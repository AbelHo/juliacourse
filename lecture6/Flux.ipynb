{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are many deep-learning libraries in Julia. One of the most used one is Flux.jl  \n",
    "*\"Relax! Flux is the ML library that doesn't make you tensor https://fluxml.ai/ \"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Flux\n",
    "model = Chain(\n",
    "  Dense(10, 5, σ),\n",
    "  Dense(5, 2),\n",
    "  softmax)\n",
    "\n",
    "model(rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Flux: Params\n",
    "\n",
    "W = rand(2, 5)\n",
    "b = rand(2)\n",
    "\n",
    "predict(x) = W*x .+ b\n",
    "loss(x, y) = sum((predict(x) .- y).^2)\n",
    "\n",
    "x, y = rand(5), rand(2) # Dummy data\n",
    "l = loss(x, y) # ~ 3\n",
    "\n",
    "θ = Params([W, b])\n",
    "grads = Flux.gradient(() -> loss(x, y), θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@show loss(x, y)\n",
    "η = 0.01 # Learning Rate\n",
    "for p in (W, b)\n",
    "  p .-= η * grads[p]\n",
    "end\n",
    "@show loss(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Running this will alter the parameters W and b and our loss should go down. Flux provides a more general way to do optimiser updates like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using Flux.Optimise: update!\n",
    "@show loss(x, y)\n",
    "opt = Descent(0.01) # Gradient descent with learning rate 0.1\n",
    "\n",
    "for p in (W, b)\n",
    "  update!(opt, p, grads[p])\n",
    "end\n",
    "@show loss(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training\n",
    "To actually train a model we need three things:\n",
    "\n",
    "- A objective function, that evaluates how well a model is doing given some input data.\n",
    "- A collection of data points that will be provided to the objective function.\n",
    "- An optimiser that will update the model parameters appropriately.\n",
    "With these we can call Flux.train!:  \n",
    "`Flux.train!(objective, params, data, opt)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "  Dense(784, 32, σ),\n",
    "  Dense(32, 10), softmax)\n",
    "\n",
    "loss(x, y) = Flux.mse(m(x), y)\n",
    "ps = Flux.params(m)\n",
    "\n",
    "# later\n",
    "Flux.train!(loss, ps, data, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Datasets\n",
    "The data argument provides a collection of data to train with (usually a set of inputs `x` and target outputs `y`). For example, here's a dummy data set with only one data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = rand(784)\n",
    "y = rand(10)\n",
    "dataset = [(x, y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Flux.train! will call `loss(x, y)`, calculate gradients, update the weights and then move on to the next data point if there is one. We can train the model on the same data three times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset = [(x, y), (x, y), (x, y)]\n",
    "# Or equivalently\n",
    "dataset = Iterators.repeated((x, y), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's common to load the xs and ys separately. In this case you can use zip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xs = [rand(784), rand(784), rand(784)]\n",
    "ys = [rand( 10), rand( 10), rand( 10)]\n",
    "dataset = zip(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that, by default, train! only loops over the data once (a single \"epoch\"). A convenient way to run multiple epochs from the REPL is provided by `@epochs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example -- Time series clustering \n",
    "Say we have a bunch of $n$ time series of length $T$ collected in matrix $A \\in \\mathbb{R^{T\\times m}}$\n",
    "\n",
    "Can we decompose $A$ as a combination of \"basis time series\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is exactly what SVD does $A = U(SV^T)$. If we keep only the $k$ largest singular value/vector pairs, we have a low-rank approximation (the number of basis time series is lower than the number of measured time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we want a sparse decomposition?\n",
    "$$A = WH$$ where $h$ is sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What if we want each time series $a_i = W h_i$ to be not only sparse in $h_i$, but that $h_i$ lives on the probability simplex\n",
    "$$h_i \\geq 0, \\quad \\sum h_i = 1$$\n",
    "This would allow us to say that $a_i$ is 10% of one base vector and 90% of another one, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dirichlet process prior on $h_i \\sim Dirichlet(\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One can solve this problem using Bayesian non-parametrics, but it takes forever (I have tried)\n",
    "\n",
    "One can also just maximize\n",
    "$$|| A - WH|| - (\\alpha -1)\\sum H$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To make sure $h_i$ stays on the probability simplex, we optimize over $\\bar h_i$ and calculate $h_i = softmax(\\bar h_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We start by creating some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "m,n,k = 20,10,3\n",
    "\n",
    "W = randn(m,k)\n",
    "H = softmax(5randn(k,n))\n",
    "A = W*H\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can try the SVD method first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "function rank_k_svd(A,k)\n",
    "    s = svd(A)\n",
    "    Wsvd, Hsvd = s.U[:,1:k], s.Vt[1:k,:]\n",
    "    Asvd = Wsvd*Diagonal(s.S[1:k])*Hsvd\n",
    "    Wsvd,Hsvd,Asvd\n",
    "end\n",
    "\n",
    "Wsvd,Hsvd,Asvd = rank_k_svd(A,k)\n",
    "@assert norm(A-Asvd)/norm(A) < 1e-10\n",
    "Hsvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The resulting $H$ is dense and hard to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now set up and solve the stated optimization problem using Flux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "α = 0.2\n",
    "Wh,Hh = randn(m,k+1), randn(k+1,n)\n",
    "p = Params((Wh,Hh))\n",
    "dir(H,α) = (α-1)*sum(log, H)\n",
    "function cost()\n",
    "    H = softmax(Hh)\n",
    "    norm(A-Wh*H) - dir(H,α) # Negative likelihood\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then create an optimizer and run gradient-based training for a number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "opt = ADAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@time for i = 1:2000\n",
    "    gs = Flux.gradient(cost, p)\n",
    "    Flux.Optimise.update!(opt, p, gs)\n",
    "end\n",
    "Ah = (Wh*softmax(Hh))\n",
    "@show norm(A-Asvd)/norm(A)\n",
    "@show norm(A-Ah)/norm(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "heatmap(A, layout=2, ylabel=\"Time\", xlabel=\"Time-series index\"); heatmap!(Ah, subplot=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How well does our decomposition approximate the original time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(A, c=:red, layout=10); plot!(Ah, c=:blue, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Just for fun, let's look at the basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(Wh, layout=size(Wh,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's see if the resulting $H$ is sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(heatmap(softmax(Hh)), heatmap(Hsvd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To see if we have used too many basis functions, we see if there are strong correlations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "heatmap(abs.(cor(Hh')))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.3.0-rc4",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
