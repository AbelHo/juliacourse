{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are many deep-learning libraries in Julia. One of the most used one is Flux.jl  \n",
    "*\"Relax! Flux is the ML library that doesn't make you tensor https://fluxml.ai/\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 2-element Array{Float32,1}:\n",
       " 0.16757292f0\n",
       " 0.832427f0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "model = Chain(\n",
    "  Dense(10, 5, σ),\n",
    "  Dense(5, 2),\n",
    "  softmax)\n",
    "\n",
    "model(rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grads(...)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux, Flux.Tracker\n",
    "\n",
    "W = param(rand(2, 5))\n",
    "b = param(rand(2))\n",
    "\n",
    "predict(x) = W*x .+ b\n",
    "loss(x, y) = sum((predict(x) .- y).^2)\n",
    "\n",
    "x, y = rand(5), rand(2) # Dummy data\n",
    "l = loss(x, y) # ~ 3\n",
    "\n",
    "θ = Params([W, b])\n",
    "grads = Tracker.gradient(() -> loss(x, y), θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using Flux.Tracker: grad, update!\n",
    "\n",
    "η = 0.1 # Learning Rate\n",
    "for p in (W, b)\n",
    "  update!(p, -η * grads[p])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Running this will alter the parameters W and b and our loss should go down. Flux provides a more general way to do optimiser updates like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "opt = Descent(0.1) # Gradient descent with learning rate 0.1\n",
    "\n",
    "for p in (W, b)\n",
    "  update!(opt, p, grads[p])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training\n",
    "To actually train a model we need three things:\n",
    "\n",
    "- A objective function, that evaluates how well a model is doing given some input data.\n",
    "- A collection of data points that will be provided to the objective function.\n",
    "- An optimiser that will update the model parameters appropriately.\n",
    "With these we can call Flux.train!:  \n",
    "`Flux.train!(objective, params, data, opt)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: data not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: data not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[8]:7"
     ]
    }
   ],
   "source": [
    "m = Chain(\n",
    "  Dense(784, 32, σ),\n",
    "  Dense(32, 10), softmax)\n",
    "\n",
    "loss(x, y) = Flux.mse(m(x), y)\n",
    "ps = Flux.params(m)\n",
    "\n",
    "# later\n",
    "Flux.train!(loss, ps, data, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Datasets\n",
    "The data argument provides a collection of data to train with (usually a set of inputs `x` and target outputs `y`). For example, here's a dummy data set with only one data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Tuple{Array{Float64,1},Array{Float64,1}},1}:\n",
       " ([0.939566, 0.000264101, 0.646587, 0.877392, 0.365939, 0.686855, 0.547145, 0.90546, 0.148181, 0.000112474  …  0.152099, 0.241316, 0.729375, 0.362262, 0.241207, 0.321071, 0.081579, 0.539145, 0.621858, 0.330482], [0.215777, 0.170623, 0.867348, 0.267223, 0.745106, 0.280507, 0.919499, 0.56173, 0.272965, 0.918508])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rand(784)\n",
    "y = rand(10)\n",
    "data = [(x, y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Flux.train! will call `loss(x, y)`, calculate gradients, update the weights and then move on to the next data point if there is one. We can train the model on the same data three times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Iterators.Take{Base.Iterators.Repeated{Tuple{Array{Float64,1},Array{Float64,1}}}}(Base.Iterators.Repeated{Tuple{Array{Float64,1},Array{Float64,1}}}(([0.939566, 0.000264101, 0.646587, 0.877392, 0.365939, 0.686855, 0.547145, 0.90546, 0.148181, 0.000112474  …  0.152099, 0.241316, 0.729375, 0.362262, 0.241207, 0.321071, 0.081579, 0.539145, 0.621858, 0.330482], [0.215777, 0.170623, 0.867348, 0.267223, 0.745106, 0.280507, 0.919499, 0.56173, 0.272965, 0.918508])), 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(x, y), (x, y), (x, y)]\n",
    "# Or equivalently\n",
    "data = Iterators.repeated((x, y), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's common to load the xs and ys separately. In this case you can use zip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Iterators.Zip{Tuple{Array{Array{Float64,1},1},Array{Array{Float64,1},1}}}((Array{Float64,1}[[0.08294, 0.608878, 0.443501, 0.94554, 0.022552, 0.664766, 0.848742, 0.879386, 0.666702, 0.715114  …  0.981416, 0.089761, 0.393361, 0.634776, 0.670174, 0.821625, 0.766537, 0.594076, 0.547374, 0.107182], [0.342504, 0.481631, 0.386382, 0.860105, 0.227784, 0.00965336, 0.240138, 0.302572, 0.30479, 0.907426  …  0.343283, 0.137797, 0.27948, 0.525597, 0.0364411, 0.793045, 0.145359, 0.458839, 0.408409, 0.76181], [0.42895, 0.977434, 0.446811, 0.68652, 0.326125, 0.77806, 0.0470123, 0.734319, 0.472213, 0.864425  …  0.879248, 0.950281, 0.958001, 0.689371, 0.54214, 0.640526, 0.299305, 0.389577, 0.954926, 0.776146]], Array{Float64,1}[[0.470541, 0.0898159, 0.826808, 0.301025, 0.632179, 0.327187, 0.0288924, 0.781422, 0.277394, 0.221102], [0.400823, 0.795952, 0.812812, 0.155874, 0.584326, 0.477461, 0.612816, 0.225555, 0.583877, 0.0566462], [0.342955, 0.255601, 0.662287, 0.571431, 0.405345, 0.824443, 0.866034, 0.975322, 0.873248, 0.841957]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [rand(784), rand(784), rand(784)]\n",
    "ys = [rand( 10), rand( 10), rand( 10)]\n",
    "data = zip(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that, by default, train! only loops over the data once (a single \"epoch\"). A convenient way to run multiple epochs from the REPL is provided by `@epochs`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
