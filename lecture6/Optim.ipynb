{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General optimization\n",
    "[Optim.jl](https://github.com/JuliaNLSolvers/Optim.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: BFGS\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999999999373614,0.999999999868622]\n",
       " * Minimum: 7.645684e-21\n",
       " * Iterations: 16\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 0.0e+00: false \n",
       "     |x - x'| = 3.48e-07 \n",
       "   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n",
       "     |f(x) - f(x')| = 9.03e+06 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: true \n",
       "     |g(x)| = 2.32e-09 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 53\n",
       " * Gradient Calls: 53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Optim\n",
    "rosenbrock(x) =  (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "result = optimize(rosenbrock, zeros(2), BFGS(), autodiff=:forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Optim automatically uses forward-mode AD to find the gradients of the cost-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`fminsearch` in matlab is similar to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Nelder-Mead\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999634355313174,0.9999315506115275]\n",
       " * Minimum: 3.525527e-09\n",
       " * Iterations: 60\n",
       " * Convergence: true\n",
       "   *  √(Σ(yᵢ-ȳ)²)/n < 1.0e-08: true\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 118"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = optimize(rosenbrock, zeros(2), NelderMead())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Newton's Method\n",
       " * Starting Point: [0.0,0.0]\n",
       " * Minimizer: [0.9999999999999994,0.9999999999999989]\n",
       " * Minimum: 3.081488e-31\n",
       " * Iterations: 14\n",
       " * Convergence: true\n",
       "   * |x - x'| ≤ 0.0e+00: false \n",
       "     |x - x'| = 3.06e-09 \n",
       "   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n",
       "     |f(x) - f(x')| = 3.03e+13 |f(x)|\n",
       "   * |g(x)| ≤ 1.0e-08: true \n",
       "     |g(x)| = 1.11e-15 \n",
       "   * Stopped by an increasing objective: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Calls: 44\n",
       " * Gradient Calls: 44\n",
       " * Hessian Calls: 14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = optimize(rosenbrock, zeros(2), Newton(), autodiff=:forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optim supports a number of common optimization algorithms\n",
    "\n",
    "- Gradient Free\n",
    "  - Nelder Mead\n",
    "  - Simulated Annealing\n",
    "  - Simulated Annealing w/ bounds\n",
    "  - Particle Swarm\n",
    "- Gradient Required\n",
    "  - Conjugate Gradient\n",
    "  - Gradient Descent\n",
    "  - (L-)BFGS\n",
    "  - Acceleration\n",
    "- Hessian Required\n",
    "  - Newton\n",
    "  - Newton with Trust Region\n",
    "  - Interior point Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optim is the goto tool if you want to optimize a Julia function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Drawbacks:\n",
    "- Only handles box constraints\n",
    "- Does not exploit structure\n",
    "- Does not explot convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Pros:\n",
    "- Very responsive developer\n",
    "- Very easy to use\n",
    "- Automatic AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Convex program -> Convex.jl\n",
    "- Structured/difficult/mixed-integer problem -> JuMP.jl\n",
    "- Nonlinear constraints -> NLopt.jl\n",
    "- Deep learning -> Flux.jl"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
